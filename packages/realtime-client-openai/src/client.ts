import { RealtimeClient } from "@charivo/realtime-core";

/**
 * OpenAI Realtime API ì´ë²¤íŠ¸ íƒ€ì…
 */
interface ServerEvent {
  type: string;
  [key: string]: any;
}

interface ClientEventOptions {
  apiEndpoint: string; // e.g., "/api/realtime"
  debug?: boolean; // Enable debug logging
}

/**
 * OpenAI Realtime API Client (WebRTC)
 *
 * WebRTCë¥¼ í†µí•´ OpenAI Realtime APIì™€ í†µì‹ í•©ë‹ˆë‹¤.
 * ì„œë²„ì˜ unified interfaceë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì—°ê²°í•©ë‹ˆë‹¤.
 *
 * ì¥ì :
 * - API í‚¤ë¥¼ í´ë¼ì´ì–¸íŠ¸ì— ë…¸ì¶œí•˜ì§€ ì•ŠìŒ
 * - ì˜¤ë””ì˜¤ëŠ” WebRTCê°€ ìë™ ì²˜ë¦¬ (ë¦½ì‹±í¬ì— í•„ìš”í•œ ë°ì´í„°ë§Œ ì¶”ì¶œ)
 * - ì´ë²¤íŠ¸ëŠ” DataChannelë¡œ ê´€ë¦¬
 */
export class OpenAIRealtimeClient implements RealtimeClient {
  private pc: RTCPeerConnection | null = null;
  private dc: RTCDataChannel | null = null;
  private audioElement: HTMLAudioElement | null = null;
  private mediaStream: MediaStream | null = null;
  private apiEndpoint: string;
  private audioContext: AudioContext | null = null;
  private analyser: AnalyserNode | null = null;
  private lipSyncInterval: number | null = null;
  private isResponseInProgress = false;
  private debug: boolean;

  // ì½œë°± í•¨ìˆ˜ë“¤
  private textDeltaCallback?: (text: string) => void;
  private audioDeltaCallback?: (base64Audio: string) => void;
  private lipSyncCallback?: (rms: number) => void; // Direct RMS callback
  private audioDoneCallback?: () => void;
  private toolCallCallback?: (name: string, args: any) => void;
  private errorCallback?: (error: Error) => void;

  constructor(options: ClientEventOptions) {
    this.apiEndpoint = options.apiEndpoint;
    this.debug = options.debug ?? false;
  }

  /**
   * Debug logging helper
   */
  private log(...args: any[]): void {
    if (this.debug) {
      console.log(...args);
    }
  }

  /**
   * WebRTC ì—°ê²° ì‹œì‘
   */
  async connect(): Promise<void> {
    try {
      console.log("ğŸ”Œ Starting WebRTC connection to Realtime API");

      // 1. Create RTCPeerConnection
      this.pc = new RTCPeerConnection();

      // 2. Set up audio playback (AI voice output)
      this.audioElement = document.createElement("audio");
      this.audioElement.autoplay = true;

      this.pc.ontrack = (e) => {
        console.log("ğŸµ Received audio track from OpenAI");
        if (this.audioElement) {
          this.audioElement.srcObject = e.streams[0];
        }

        // Set up audio analysis for lip sync
        this.setupAudioAnalysis(e.streams[0]);
      };

      // 3. Add local audio track (microphone input)
      try {
        this.mediaStream = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        const audioTrack = this.mediaStream.getTracks()[0];
        this.pc.addTrack(audioTrack, this.mediaStream);
        console.log("ğŸ¤ Added microphone track");
      } catch (error) {
        console.error("âŒ Failed to get microphone access:", error);
        throw new Error("Microphone access required for Realtime API");
      }

      // 4. Set up data channel for events
      this.dc = this.pc.createDataChannel("oai-events");

      this.dc.onopen = () => {
        console.log("ğŸ“¡ DataChannel opened");
      };

      this.dc.onmessage = (e) => {
        const event = JSON.parse(e.data);
        this.handleServerEvent(event);
      };

      this.dc.onerror = (error) => {
        console.error("âŒ DataChannel error:", error);
        this.errorCallback?.(new Error("DataChannel error"));
      };

      // 5. Create SDP offer
      const offer = await this.pc.createOffer();
      await this.pc.setLocalDescription(offer);

      console.log("ğŸ“¤ Sending SDP offer to server");

      // 6. Send SDP offer to server (unified interface)
      const sdpResponse = await fetch(this.apiEndpoint, {
        method: "POST",
        headers: {
          "Content-Type": "application/sdp",
        },
        body: offer.sdp,
      });

      if (!sdpResponse.ok) {
        const errorText = await sdpResponse.text();
        throw new Error(`Failed to create Realtime session: ${errorText}`);
      }

      // 7. Set remote description (SDP answer from OpenAI)
      const sdpAnswer = await sdpResponse.text();
      const answer: RTCSessionDescriptionInit = {
        type: "answer",
        sdp: sdpAnswer,
      };
      await this.pc.setRemoteDescription(answer);

      console.log("âœ… WebRTC connection established");
    } catch (error) {
      console.error("âŒ WebRTC connection error:", error);
      this.cleanup();
      throw error;
    }
  }

  /**
   * WebRTC ì—°ê²° ì¢…ë£Œ
   */
  async disconnect(): Promise<void> {
    console.log("ğŸ”Œ Disconnecting WebRTC");
    this.cleanup();
  }

  /**
   * í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡
   */
  async sendText(text: string): Promise<void> {
    if (!this.dc || this.dc.readyState !== "open") {
      throw new Error("DataChannel not ready");
    }

    if (this.isResponseInProgress) {
      console.warn("âš ï¸ Response already in progress, skipping request");
      return;
    }

    // conversation.item.create
    const createEvent = {
      type: "conversation.item.create",
      item: {
        type: "message",
        role: "user",
        content: [
          {
            type: "input_text",
            text,
          },
        ],
      },
    };

    this.dc.send(JSON.stringify(createEvent));

    // response.create
    const responseEvent = {
      type: "response.create",
    };

    this.dc.send(JSON.stringify(responseEvent));
    this.isResponseInProgress = true;

    console.log("ğŸ“¤ Sent text message:", text);
  }

  /**
   * ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡
   *
   * Note: WebRTCëŠ” ë§ˆì´í¬ ì˜¤ë””ì˜¤ë¥¼ ìë™ìœ¼ë¡œ ì „ì†¡í•˜ë¯€ë¡œ
   * ì´ ë©”ì„œë“œëŠ” ì‚¬ìš©ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   */
  async sendAudio(_audio: ArrayBuffer): Promise<void> {
    // WebRTCëŠ” ìë™ìœ¼ë¡œ ë§ˆì´í¬ ì˜¤ë””ì˜¤ë¥¼ ì „ì†¡
    console.warn(
      "sendAudio is not needed with WebRTC - audio is automatically transmitted",
    );
  }

  /**
   * ì½œë°± ë“±ë¡
   */
  onTextDelta(callback: (text: string) => void): void {
    this.textDeltaCallback = callback;
  }

  onAudioDelta(callback: (base64Audio: string) => void): void {
    this.audioDeltaCallback = callback;
  }

  onLipSyncUpdate(callback: (rms: number) => void): void {
    this.lipSyncCallback = callback;
  }

  onAudioDone(callback: () => void): void {
    this.audioDoneCallback = callback;
  }

  onToolCall(callback: (name: string, args: any) => void): void {
    this.toolCallCallback = callback;
  }

  onError(callback: (error: Error) => void): void {
    this.errorCallback = callback;
  }

  /**
   * ì„œë²„ ì´ë²¤íŠ¸ ì²˜ë¦¬
   */
  private handleServerEvent(event: ServerEvent): void {
    // Debug logging
    if (this.debug && !event.type.includes("audio.delta")) {
      this.log("ğŸ“¡ [Realtime Event]:", event.type, event);
    }

    switch (event.type) {
      case "session.created":
      case "session.updated":
        this.log("ğŸ“¡ Session:", event.type);
        break;

      case "response.audio.delta":
        // WebRTCì—ì„œëŠ” ì˜¤ë””ì˜¤ê°€ ìë™ìœ¼ë¡œ ì¬ìƒë˜ì§€ë§Œ,
        // ë¦½ì‹±í¬ë¥¼ ìœ„í•´ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì½œë°±ìœ¼ë¡œ ì „ë‹¬
        if (event.delta) {
          this.audioDeltaCallback?.(event.delta);
        }
        break;

      case "response.audio.done":
        // ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
        this.audioDoneCallback?.();
        break;

      case "response.done":
        // ì‘ë‹µ ì™„ë£Œ - ìƒˆë¡œìš´ ìš”ì²­ ê°€ëŠ¥
        this.isResponseInProgress = false;
        this.log("âœ… Response completed, ready for next request");
        break;

      case "response.audio_transcript.delta":
        // í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° (AI ì‘ë‹µ)
        if (event.delta) {
          this.textDeltaCallback?.(event.delta);
        }
        break;

      case "conversation.item.input_audio_transcription.completed":
        // ì‚¬ìš©ì ìŒì„± ì¸ì‹ ì™„ë£Œ
        this.log("ğŸ¤ User transcript:", event.transcript);
        break;

      case "response.function_call_arguments.done":
        // Tool call completed
        this.handleToolCallDone(event);
        break;

      case "error":
        console.error("âŒ Realtime API error:", event.error);
        this.errorCallback?.(
          new Error(event.error?.message || "Unknown error"),
        );
        // ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë‹¤ìŒ ìš”ì²­ì„ í—ˆìš©
        this.isResponseInProgress = false;
        break;

      default:
        // ê¸°íƒ€ ì´ë²¤íŠ¸ëŠ” ë¡œê·¸ë§Œ ì¶œë ¥
        if (
          this.debug &&
          (event.type.startsWith("response.") ||
            event.type.startsWith("conversation."))
        ) {
          this.log("ğŸ“¡ Event:", event.type);
        }
        break;
    }
  }

  /**
   * Tool call ì™„ë£Œ ì²˜ë¦¬
   */
  private handleToolCallDone(event: ServerEvent): void {
    const callId =
      (event.call_id as string | undefined) ||
      (event.item?.call_id as string | undefined) ||
      (event.item_id as string | undefined);

    const name =
      (event.name as string | undefined) ||
      (event.item?.name as string | undefined);

    // arguments field contains the complete JSON string
    const argsJson =
      (event.arguments as string | undefined) ||
      (event.item?.arguments as string | undefined);

    this.log(
      "ğŸ”§ [Tool Done] callId:",
      callId,
      "name:",
      name,
      "argsJson:",
      argsJson,
    );

    if (!name || !argsJson) {
      console.warn("âš ï¸ Tool call done but missing name or arguments");
      return;
    }

    let args: any = {};
    try {
      args = JSON.parse(argsJson);
    } catch (e) {
      console.error("Failed to parse tool call args", e, argsJson);
      return;
    }

    this.log(`ğŸ”§ Tool call: ${name}`, args);
    this.toolCallCallback?.(name, args);

    // Send tool output back to OpenAI to continue the conversation
    if (callId && this.dc && this.dc.readyState === "open") {
      this.sendToolOutput(callId, { success: true });
    }
  }

  /**
   * Send tool output back to OpenAI
   */
  private sendToolOutput(callId: string, output: any): void {
    if (!this.dc || this.dc.readyState !== "open") {
      console.warn("âš ï¸ Cannot send tool output: DataChannel not ready");
      return;
    }

    const outputEvent = {
      type: "conversation.item.create",
      item: {
        type: "function_call_output",
        call_id: callId,
        output: JSON.stringify(output),
      },
    };

    this.dc.send(JSON.stringify(outputEvent));
    this.log("ğŸ“¤ Sent tool output for call:", callId);

    // Request a new response to continue the conversation
    const responseEvent = {
      type: "response.create",
    };

    this.dc.send(JSON.stringify(responseEvent));
    this.log("ğŸ“¤ Requested new response after tool call");
  }

  /**
   * ì˜¤ë””ì˜¤ ë¶„ì„ ì„¤ì • (ë¦½ì‹±í¬ìš©)
   */
  private setupAudioAnalysis(stream: MediaStream): void {
    try {
      // AudioContext ìƒì„±
      this.audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();

      // MediaStream â†’ AudioContext
      const source = this.audioContext.createMediaStreamSource(stream);

      // AnalyserNode ìƒì„±
      this.analyser = this.audioContext.createAnalyser();
      this.analyser.fftSize = 256;
      this.analyser.smoothingTimeConstant = 0.8;

      // Connect: source â†’ analyser (not to destination, just analyze)
      source.connect(this.analyser);

      console.log("ğŸµ Audio analysis setup complete for lip sync");

      // Start analyzing audio for lip sync
      this.startLipSyncAnalysis();
    } catch (error) {
      console.error("Failed to setup audio analysis:", error);
    }
  }

  /**
   * ë¦½ì‹±í¬ ë¶„ì„ ì‹œì‘
   */
  private startLipSyncAnalysis(): void {
    if (!this.analyser) return;

    const bufferLength = this.analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    // 60fpsë¡œ RMS ê³„ì‚°
    this.lipSyncInterval = window.setInterval(() => {
      if (!this.analyser) return;

      this.analyser.getByteFrequencyData(dataArray);

      // Calculate RMS from frequency data
      let sum = 0;
      for (let i = 0; i < bufferLength; i++) {
        const normalized = dataArray[i] / 255; // Normalize to 0-1
        sum += normalized * normalized;
      }

      const rms = Math.sqrt(sum / bufferLength);

      // Amplify and clamp (similar to existing lip sync)
      const amplifiedRms = Math.min(rms * 3, 1.0);

      // Send RMS directly to lip sync callback
      if (this.lipSyncCallback) {
        this.lipSyncCallback(amplifiedRms);
      }
    }, 1000 / 60); // 60fps
  }

  /**
   * ë¦½ì‹±í¬ ë¶„ì„ ì¤‘ì§€
   */
  private stopLipSyncAnalysis(): void {
    if (this.lipSyncInterval) {
      clearInterval(this.lipSyncInterval);
      this.lipSyncInterval = null;
    }

    // Send final RMS 0 to close mouth
    if (this.lipSyncCallback) {
      this.lipSyncCallback(0);
    }
  }

  /**
   * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   */
  private cleanup(): void {
    this.stopLipSyncAnalysis();

    if (this.dc) {
      this.dc.close();
      this.dc = null;
    }

    if (this.pc) {
      this.pc.close();
      this.pc = null;
    }

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.audioElement) {
      this.audioElement.srcObject = null;
      this.audioElement = null;
    }

    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }

    this.analyser = null;
    this.isResponseInProgress = false;
  }
}

/**
 * OpenAI Realtime Client ìƒì„± í—¬í¼ í•¨ìˆ˜
 */
export function createOpenAIRealtimeClient(
  options: ClientEventOptions,
): RealtimeClient {
  return new OpenAIRealtimeClient(options);
}
